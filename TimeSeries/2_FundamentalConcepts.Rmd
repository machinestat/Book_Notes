---
title: "FUNDAMENTAL CONCEPTS"
output: pdf_document
---

## 2.1 Time Series and Stochastic Processes

The sequence of random variables $\left \{ Y_{t}:t = 0, \pm 1, \pm2, \pm3, \cdots  \right \}$ is called a **stachastic process** and serves as a model for an observed time series. The complete probalistic structure of such a process is determined by the set distributions of all finite collections of the $Y's$, and much of the information in these joint distributions can be described in terms of means, variances, and covariances. If the joint distributions of the Y's are multivariate normal distributions, then the first and second moments completely determine all the join distributions.

## 2.2 Means, Variances, and Covariances

For a stochastic process $\left \{ Y_{t}:t = 0, \pm 1, \pm2, \pm3, \cdots  \right \}$, the **mean function** defined by

$$
\mu _{t}= E\left ( Y_{t} \right ) \text{for} \ t = 0, \pm1, \pm2, \cdots \ \text{(2.2.1)}
$$

$\mu_{t}$ is just the expected value of the process at time t. In general, $\mu_{t}$ can be different at each time point t.

The **autocovariance function**, $\gamma _{t, s}$, is defined as

$$
\gamma_{t, s} = Cov\left ( Y_{t}, Y_{s} \right ) \text{for} \ t, s = 0, \pm1, \pm2, \cdots \ \text{(2.2.2)}
$$

where $Cov\left ( Y_{t}, Y_{s} \right ) = E\left [ \left ( Y_{t} - \mu_{t} \right ) \left ( Y_{s} - \mu_{s} \right )\right ] = E\left ( Y_{t}Y_{s} \right ) - \mu_{t}\mu_{s}$

The **autocorrelation function**, $\rho _{t,s}$, is given by

$$
\rho_{t, s} = Corr\left( Y_{t}, Y_{s} \right) \text{for} \ t, s = 0, \pm1, \pm2, \cdots \ \text{(2.2.3)}
$$

where

$$
Corr \left( Y_{t}, Y_{s} \right) = \frac{Cov\left ( Y_{t}, Y_{s} \right )}{\sqrt{Var\left ( Y_{t} \right ) Var\left ( Y_{s} \right )}} = \frac{\gamma _{t, s}}{\sqrt{\gamma _{t, t} \gamma _{s, s}}} \ \ \text{(2.2.4)}
$$

Both the covariance and correlation are measures of the (linear) dependence between random variables but the unitless correlation is easier to interpret. The following properties follow from known results and definitions:

$$
\left.\begin{matrix}
\begin{aligned} 
&\gamma _{t, t} =Var\left ( Y_{t} \right )   && \rho _{t, t} = 1\\ 
&\gamma _{t, s} =\gamma _{s, t}              && \rho _{t, s} = \rho _{s, t}\\            
& \left | \gamma _{t, s} \leq  \sqrt{\gamma _{t, t}\gamma _{s, s}} \right | &&\left | \rho _{t, s} \leqslant  1 \right |
\end{aligned}
\end{matrix}\right\}
\ \text{(2.2.5)}
$$

values of $\rho _{t, s}$ near $\pm 1$ indicate strong (linear) dependence, whereas values near zero indicate weak (linear) dependence. If $\rho _{t, s} = 0$, then $Y_{t}$ and $Y_{s}$ are *uncorrelated*.

To investigate the covariance properties of various time series models, the following results will be using repeatedly: If $c_{1}, c_{2}, \cdots, c_{m}$ and $d_{1}, d_{2}, \cdots, d_{n}$ are constants and $t_{1}, t_{2}, \cdots, t_{m}$ and $s_{1}, s_{2}, \cdots, s_{n}$ are time points, then

$$ 
Cov\left [ \sum_{i = 1}^{m}c_{i}Y_{t_{i}}, \sum_{j = 1}^{n}d_{j}Y_{s_{j}} \right ] = \sum_{j=1}^{n}c_{i}d_{j}Cov\left ( Y_{t_{i}}, Y_{s_{j}} \right ) \ \text{(2.2.6)}
$$

$$
Var\left [ \sum_{i = 1}^{n} c_{i}Y_{t_{i}} \right ] = \sum_{i = 1}^{n}c_{i}^{2}Var(Y_{t_{i}}) + 2\sum_{i = 2}^{n}\sum_{j = 1}^{i - 1}c_{i}c_{j}Cov\left ( Y_{t_{i}}, Y_{t_{j}} \right ) \ \text{(2.2.7)}
$$

**The Random Walk**

Let $e_{1}, e_{2}, \cdots $ be a sequence of independent, identically distributed random variables each with 0 mean and variance $\sigma _{e}^{2}$. The observed time series, ${Y_{t}: t = 1, 2, \cdots}$, is constructed as follows:

$$
\left.\begin{matrix}
\begin{aligned} 
Y_{1} &= e_{1}\\ 
Y_{2} &= e_{1} + e_{2}\\ 
\vdots \\
Y_{t} &= e_{1} + e_{2} + \cdots + e_{t}
\end{aligned}
\end{matrix}\right\}  \ \text{(2.2.8)}
$$

Alternatively,

$$
Y_{t} = Y_{t-1} + e_{t}  \ \ \text{(2.2.9)}
$$

with "initial condition" $Y_{1} = e_{1}$. If $e$'s are interpreted as the sizes the "steps" take (forward or backward) along a number line, then $Y_{t}$ is the position of the "random walker" at time t. From the Equation (2.2.8), we obtain the mean function.

$$
\begin{aligned} 
\mu_{t} &= E(Y_{t}) = E(e_{1} + e_{2} + \cdots + e_{t}) = E(e_{1}) + E(e_{2}) + \cdots + E(e_{t}) \\
        &= 0 + 0 + \cdots + 0
\end{aligned}
$$

so that $\mu_{t} = 0$  for all $t$.                    (2.2.10)

also,

$$
\begin{aligned}
Var(Y_{t}) &= Var(e_{1} + e_{2} + \cdots + e_{t}) = Var(e_{1}) + Var(e_{2}) + \cdots + Var(e_{t}) \\
           &= \sigma^{2} + \sigma^{2} + \cdots + \sigma^{2} \\
           &= t\sigma^{2}  \ \ \text{(2.2.11)}
\end{aligned}
$$
Notice that the process variance increases linearly with time.

For the covariance function, suppose that $1 \leq t \leq s$, then

$$
\gamma _{t, s} = Cov(Y_{t}, Y_{s}) = Cov(e_{1} + e_{2} + \cdots + e_{t}, e_{1} + e_{2} + \cdots + e_{t} + e_{t+1} + \cdots + e_{s})
$$

From equation (2.2.6), we have

$$
\gamma _{t, s} = \sum _{i = 1}^{s} \sum _{j=1}^{t}Cov(e_{i}, e_{j})
$$

These covariances are zeor unless $i = j$, in which case they equal $Var(e_{i}) = \sigma_{e}^{2}$. There are exactly $t$ of these so that $\gamma _{t, s} = t \sigma_{e}^{2}$.

Since $\gamma _{t, s} = \gamma _{s, t}$, this specifies autocovariance function for all time points $t$ and $s$ and we can write

$$
\gamma _{t, s} = t \sigma_{e}^{2} \ \ \text{for} \ \ 1 \leq t \leq s \ \ \text{(2.2.12)}
$$

The autocorrelation function for random walk is

$$
\rho _{t, s} = \frac{\gamma _{t, s}}{\sqrt{\gamma _{t, t}\gamma _{s,s}}} = \sqrt{\frac{t}{s}}  \ \ \text{for} \ \ \ 1 \leq t \leq s \ \ \text{(2.2.13)}
$$

The following numerical values help us understand the behavior of the random walk.

$$
\begin{matrix}
\begin{aligned}
\rho_{1,2} &= \sqrt{\frac{1}{2}} = 0.707 & \rho_{8,9} &= \sqrt{\frac{8}{9}} = 0.943\\ 
\rho_{24,25} &= \sqrt{\frac{24}{25}} = 0.980 & \rho_{1,25} &= \sqrt{\frac{1}{25}} = 0.2
\end{aligned}
\end{matrix}
$$

The values of $Y$ at neighboring time points are more and more strongly and positively correlated as time goes by, and the values of $Y$ at distant time points are less and less corrlated.

```{r message=FALSE, warning=FALSE}
# Exhibit 2.1 Time Series Plot of a Random Walk
library(TSA)
data(rwalk)  # rwalk contains a simulated random walk
plot(rwalk, type = 'o', ylab = "Random Walk")
```

**A Moving Average**

Suppose $\left \{ Y_{t} \right \}$ is constructed as

$$
Y_{t} = \frac{e_{t} + e_{t - 1}}{2}  \  \  \  \ \text{(2.2.14)}
$$

where the $e$'s are assumed to be independent and identically distributed with 0 mean and variance $\sigma_{e}^{2}$. Here  
$$
\begin{aligned}
\mu_{t} &= E\left ( Y_{t} \right ) = E \left( \frac{e_{t} + e_{t - 1}}{2} \right) = \frac{E\left( e_{t}\right) + E\left( e_{t-1} \right)}{2}\\     
        &= 0
\end{aligned}
$$

and

$$
\begin{aligned}
Var \left( Y_{t} \right) &= Var \frac{\left( e_{t} + e_{t-1}\right)}{2} = \frac{Var\left( e_{t}\right) + Var\left( e_{t-1}\right)}{4} \\
                         &= 0.5\sigma_{e}^{2}
\end{aligned}
$$ 

Also

$$
\begin{aligned}
Cov\left( Y_{t}, Y_{t-1} \right) &= Cov \left \{ \frac{e_{t} + e_{t-1}}{2}, \frac{e_{t-1} + e_{t-2}}{2} \right\} \\
&= \frac{Cov\left( e_{t}, e_{t-1}\right) + Cov\left( e_{t}, e_{t-2}\right) + Cov\left( e_{t-1}, e_{t-1}\right) + Cov\left( e_{t-1}, e_{t-2} \right)}{4}\\
&= \frac{e_{t-1}, e_{t-1}}{4} \\
&= 0.25\sigma_{e}^{2}
\end{aligned}
$$

or $\gamma_{t, t-1} = 0.25\sigma_{e}^{2}$  for all $t$  
Furthermore, 

$$
\begin{aligned}
Cov \left( Y_{t}, Y_{t-2}\right) &= Cov \left \{ \frac{e_{t} + e_{t-1}}{2}, \frac{e_{t-2} + e_{t-3}}{2} \right\} \\
&= 0 \ \ \text{since e's are independent}
\end{aligned}
$$

Similarly, $Cov(Y_{t}, Y_{t-k}) = 0$ for $k > 1$, so we may write

$$
\begin{aligned}
\gamma = \left\{\begin{matrix}
0.5\sigma_{e}^{2}  &\text{for}  \left | t - s \right | = 0\\ 
0.25\sigma_{e}^{2}  &\text{for}  \left | t - s \right | = 1\\ 
0   &\text{for} \left | t - s \right | > 1
\end{matrix}\right.
\end{aligned}
$$

The autocorrelation is:

$$
\begin{aligned}
\rho_{t, s} = \left\{\begin{matrix}
1 \ &\text{for} \left | t - s \right | = 0\\ 
0.5 \ &\text{for} \left | t - s \right | = 1\\ 
0 \ &\text{for} \left | t - s \right | > 1
\end{matrix}\right. \  \  \text{(2.2.16)}
\end{aligned}
$$

since $0.25\sigma_{e}^{2}/0.5\sigma_{e}^{2} = 0.5$.

Values of $Y$ precisely one time unit apart have exactly the same correlation no matter where they occur in time. Furthermore, $\rho_{t, t-k}$ is the same for all values of $t$.

## 2.3 Stationarity

The basic idea of stationarity is that the probability laws that govern the behavior of the process do not change over time. Specifically, a process $\left \{ Y_{t} \right \}$ is said to be **strictly stationary** if the joint distribution of $Y_{t_{1}}, Y_{t_{2}}, \cdots, Y_{t_{n}}$ is the same as the joint distribution of $Y_{t_{1} - k}, Y_{t_{2} - k}, \cdots, Y_{t_{n} - k}$ for all choices of time points $t_{1}, t_{2}, \cdots, t_{n}$ and all choices of time lag $k$.

When $n = 1$ the (univariate) distribution of $Y_{t}$ is the same as that of $Y_{t - k}$ for all $t$ and $k$; so the $Y$'s are (marginally) identically distributed. It then follows that $E(Y_{t}) = E(Y_{t - k})$ for all $t$ and $k$ so that the mean function is constant for all time. Additionally, $Var(Y_{t}) = Var(Y_{t - k})$ for all $t$ and $k$ so that variance is also constant over time.

Setting $n = 2$ in the stationary definition, the bivariate distribution of $Y_{t}$ and $Y_{s}$ must be the same as that of $Y_{t - k}$ and $Y_{s - k}$ from which it follows that $Cov(Y_{t}, Y_{s}) = Cov(Y_{t - k}, Y_{t - s})$ for all $t, s$ and $k$. Putting $k = s$ and then $k = t$, we obtain

$$
\begin{aligned}
\gamma _{t, s} &= Cov(Y_{t - s}, Y_{0}) \\
               &= Cov(Y_{0}, Y_{s - t}) \\
               &= Cov(Y_{0}, Y_{\left | t - s \right|}) \\
               &= \gamma _{0, \left | t - s \right |}
\end{aligned}
$$

That is, the covariance between $Y_{t}$ and $Y_{s}$ depends on time only through the time difference $\left | t - s \right |$. Thus, for a stationary process

$$
\gamma_{k} = Cov(Y_{t}, Y_{t - k}) \ \ \text{and } \rho_{k} = Corr(Y_{t}, Y_{t - k}) \ \ \text{(2.3.1)}
$$

also

$$
\rho_{k} = \frac{\gamma_{k}}{\gamma_{0}}
$$

The general properties become

$$
\begin{aligned}
\left.\begin{matrix}
&\gamma _{0} = Var\left ( Y_{t} \right ) \ \ &\rho_{0} = 1\\ 
&\gamma _{k} = \gamma _{-k} \ \ \ &\rho _{k} = \rho _{-k}\\ 
&\left | \gamma _{k} \right | \leq \gamma _{k} \ \ \ &\left | \rho  \right | \leq 1
\end{matrix}\right\}
\end{aligned}
$$

If a process is strictly stationary and has finite variance, then the covariance function must depend only on the time lag.

A stochastic process $\left \{ Y_{t} \right \}$ is said to be **weakly** (or **second-order**) **stationary** if

*  The mean function is constant over time
*  $\gamma _{t, t - k} = \gamma _{0, k}$ for all time $t$ and lag $k$

**White Noise**

**White noise** is defined as a sequence of independent, identically distributed random variables $\left \{ e_{t} \right \}$. Its importance stems from that many useful processes can be constructed from white noise, and $\left \{ e_{t} \right \}$ is strictly stationary;

$$
\begin{aligned}
& Pr(e_{t_{1}} \leq x_{1}, e_{t_{2}} \leq x_{2}, \cdots, e_{t{n}} \leq x_{n}) \\
&= Pr(e_{t_{1}} \leq x_{1})Pr(e_{t_{2}} \leq x_{2}) \cdots Pr(e_{t{n}} \leq x_{n}) \ \ \text{(by independence)} \\
&= Pr(e_{t_{1} - k} \leq x_{1})Pr(e_{t_{2} - k} \leq x_{2}) \cdots Pr(e_{t{n} - k} \leq x_{n}) \ \ \text{(identical distributions)} \\
&= Pr(e_{t_{1} - k} \leq x_{1}, e_{t_{2} - k} \leq x_{2}, \cdots, e_{t{n} - k} \leq x_{n}) \ \ \text{(by independence)}
\end{aligned}
$$

as required. Also, $\mu_{t} = E(e_{t})$ is constant and 
$$
\begin{aligned}
\gamma_{k} = \left\{\begin{matrix}
Var(e_{t}) \ & \text{for} \ \ k = 0\\ 
0          \  & \text{for} \ \ k \neq 0
\end{matrix}\right.
\end{aligned}
$$

Alternatively, 

$$ 
\begin{aligned}
\rho_{k} = \left\{\begin{matrix}
1 \ & \text{for} \ \ k = 0\\ 
0 \  & \text{for} \ \ k \neq 0
\end{matrix}\right.
\end{aligned}
$$

The term white noise arises from the fact that a frequency analysis of the model shows that, in analogy with white light, all frequencies enter equally. We usually assume that the white noise process has mean zero and denote $Var(e_{t})$ by $\sigma_{e}^{2}$.

The moving average example, where $Y_{t} = (e_{t} + e_{t - 1})/2$, is another example of a stationary process constructed from white noise. We have for the moving average process that

$$
\begin{aligned}
\rho_{k} = \left\{\begin{matrix}
1  \   & \text{for} \ \ k = 0\\ 
0.5 \  & \text{for} \ \ \left | k \right | = 1\\
0   \  & \text{for} \ \ \left | k \right | \geq 2
\end{matrix}\right.
\end{aligned}
$$

Random Cosine Wave

Consider the process defined as follows:

$$
Y_{t} = \cos \left [ 2\pi \left ( \frac{t}{12} + \Phi  \right ) \right ] \ \ \text{for} \ \ t = 0, \pm 1, \pm2, \cdots
$$

where $\Phi $ is selected (once) from a uniform distribution on the interval from 0 to 1. A sample from such a process will apear highly deterministic since $Y_{t}$ will repeat itself identically every 12 time units and look like a perfect (discrete time) cosine curve. However, its maximum will not occur at $t = 0$ but will determined by the random phase $\Phi$. The phase $\Phi$ can be interpreted as the fraction of a complete circle completed by time $t = 0$. The statistical properties can be computed as follows:

$$
\begin{aligned}
E(Y_{t}) &= E \left \{ \cos \left [ 2\pi \left (  \frac {t}{12} + \Phi \right ) \right ] \right \} \\
         &= \int_{1}^{0}\cos \left [ 2\pi \left ( \frac{t}{12} + \phi \right ) \right ]d\phi \\
         &= \frac{1}{2\pi }\sin \left [ 2\pi \left ( \frac{t}{12} + \phi \right ) \right ]  |_{\phi =0}^{1} \\
         &= \frac{1}{2\pi }\left [ \sin \left ( 2\pi \frac{t}{12} + 2\pi \right ) - \sin \left ( 2\pi \frac{t}{12} \right )\right ]
\end{aligned} 
$$

But this is zero since the sines must agree. So $\mu_{t} = 0$ for all $t$.  
Also, 

$$
\begin{aligned}
\gamma _{t, s} &= E\left \{ \cos \left [ 2\pi \left ( \frac{t}{12} + \Phi \right ) \right ] \cos \left [ 2\pi \left ( \frac{s}{12} + \Phi \right ) \right ]\right \} \\
               &= \int_{0}^{1}\cos \left [ 2\pi \left ( \frac{t}{12} + \phi \right ) \right ]\cos \left [ 2\pi \left ( \frac{s}{12} + \phi \right ) \right ]d\phi \\
               &= \frac{1}{2}\int_{0}^{1}\left \{ \cos 2\pi \left ( \frac{t-s}{12} \right ) + \cos 2\pi \left ( \frac{t+s}{12} + 2\phi \right ) \right \}d\phi \\
               &= \frac{1}{2}\cos \left [ 2\pi \left ( \frac{\left | t - s \right |}{12} \right ) \right ]
\end{aligned} 
$$

So the process is stationary with autocorrelation function

$$
\rho_{k} = \cos\left( 2\pi\frac{k}{12} \right) \  \  \text{for} \ \ k = 0, \pm 1, \pm 2, \cdots \ \ \text{(2.3.4)}
$$

**EXERCISES**
